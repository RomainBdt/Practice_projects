{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Sources:\n","1 - https://karpathy.ai/zero-to-hero.html  \n","2 - https://www.youtube.com/watch?v=kCc8FmEb1nY  \n","3 - https://github.com/karpathy/minbpe/tree/master  "]},{"cell_type":"markdown","metadata":{},"source":["# Import libraries"]},{"cell_type":"code","execution_count":93,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T09:25:01.248595Z","iopub.status.busy":"2024-02-26T09:25:01.248278Z","iopub.status.idle":"2024-02-26T09:25:01.256637Z","shell.execute_reply":"2024-02-26T09:25:01.255806Z","shell.execute_reply.started":"2024-02-26T09:25:01.248567Z"},"trusted":true},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn as nn\n","from torch.nn import functional as F\n","from tqdm import tqdm\n","import time"]},{"cell_type":"markdown","metadata":{},"source":["# Parameters"]},{"cell_type":"code","execution_count":94,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T09:25:01.258001Z","iopub.status.busy":"2024-02-26T09:25:01.257729Z","iopub.status.idle":"2024-02-26T09:25:01.267343Z","shell.execute_reply":"2024-02-26T09:25:01.266449Z","shell.execute_reply.started":"2024-02-26T09:25:01.257979Z"},"trusted":true},"outputs":[],"source":["# Dataset parameters\n","train_ratio = 0.9\n","sample_size = 10\n","\n","SEED = 15"]},{"cell_type":"markdown","metadata":{},"source":["# Load files"]},{"cell_type":"code","execution_count":95,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T09:25:01.296859Z","iopub.status.busy":"2024-02-26T09:25:01.296541Z","iopub.status.idle":"2024-02-26T09:25:09.594297Z","shell.execute_reply":"2024-02-26T09:25:09.593329Z","shell.execute_reply.started":"2024-02-26T09:25:01.296836Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Resolving data files: 100%|██████████| 17/17 [00:01<00:00,  9.05it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Dataset size: 2564646\n"]}],"source":["from datasets import load_dataset\n","\n","# Define the path to the dataset\n","dataset_name = \"20231101.fr\"\n","\n","# Load the dataset\n","raw_dataset = load_dataset(\"wikimedia/wikipedia\", dataset_name)\n","print(f\"Dataset size: {raw_dataset['train'].num_rows}\")"]},{"cell_type":"markdown","metadata":{},"source":["# Data preparation"]},{"cell_type":"code","execution_count":96,"metadata":{},"outputs":[{"ename":"OSError","evalue":"[WinError 1224] L’opération demandée n’a pu s’accomplir sur un fichier ayant une section mappée utilisateur ouverte","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mFileExistsError\u001b[0m                           Traceback (most recent call last)","File \u001b[1;32mc:\\Users\\hzf04d\\Anaconda3\\envs\\llm2\\Lib\\shutil.py:886\u001b[0m, in \u001b[0;36mmove\u001b[1;34m(src, dst, copy_function)\u001b[0m\n\u001b[0;32m    885\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 886\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrename\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_dst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    887\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n","\u001b[1;31mFileExistsError\u001b[0m: [WinError 183] Impossible de créer un fichier déjà existant: 'C:\\\\Users\\\\hzf04d\\\\.cache\\\\huggingface\\\\datasets\\\\wikimedia___wikipedia\\\\20231101.fr\\\\0.0.0\\\\b04c8d1ceb2f5cd4588862100d08de323dccfbaa\\\\tmpcszugflp' -> 'C:\\\\Users\\\\hzf04d\\\\.cache\\\\huggingface\\\\datasets\\\\wikimedia___wikipedia\\\\20231101.fr\\\\0.0.0\\\\b04c8d1ceb2f5cd4588862100d08de323dccfbaa\\\\cache-0d3a8246fbb7490a.arrow'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)","Cell \u001b[1;32mIn[96], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m     train_sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(sample_size \u001b[38;5;241m*\u001b[39m train_ratio)\n\u001b[0;32m      7\u001b[0m     test_sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(sample_size \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39m train_ratio))\n\u001b[1;32m----> 9\u001b[0m ds_train_test \u001b[38;5;241m=\u001b[39m \u001b[43mraw_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSEED\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m train_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([t[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m ds_train_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n","File \u001b[1;32mc:\\Users\\hzf04d\\Anaconda3\\envs\\llm2\\Lib\\site-packages\\datasets\\arrow_dataset.py:557\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    550\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    551\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[0;32m    555\u001b[0m }\n\u001b[0;32m    556\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 557\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    558\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    559\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\hzf04d\\Anaconda3\\envs\\llm2\\Lib\\site-packages\\datasets\\fingerprint.py:481\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    477\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[0;32m    479\u001b[0m \u001b[38;5;66;03m# Call actual function\u001b[39;00m\n\u001b[1;32m--> 481\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:  \u001b[38;5;66;03m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\hzf04d\\Anaconda3\\envs\\llm2\\Lib\\site-packages\\datasets\\arrow_dataset.py:4520\u001b[0m, in \u001b[0;36mDataset.train_test_split\u001b[1;34m(self, test_size, train_size, shuffle, stratify_by_column, seed, generator, keep_in_memory, load_from_cache_file, train_indices_cache_file_name, test_indices_cache_file_name, writer_batch_size, train_new_fingerprint, test_new_fingerprint)\u001b[0m\n\u001b[0;32m   4517\u001b[0m         test_indices \u001b[38;5;241m=\u001b[39m permutation[:n_test]\n\u001b[0;32m   4518\u001b[0m         train_indices \u001b[38;5;241m=\u001b[39m permutation[n_test : (n_test \u001b[38;5;241m+\u001b[39m n_train)]\n\u001b[1;32m-> 4520\u001b[0m train_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4521\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4522\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindices_cache_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_indices_cache_file_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_fingerprint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_new_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4526\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4527\u001b[0m test_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselect(\n\u001b[0;32m   4528\u001b[0m     indices\u001b[38;5;241m=\u001b[39mtest_indices,\n\u001b[0;32m   4529\u001b[0m     keep_in_memory\u001b[38;5;241m=\u001b[39mkeep_in_memory,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4532\u001b[0m     new_fingerprint\u001b[38;5;241m=\u001b[39mtest_new_fingerprint,\n\u001b[0;32m   4533\u001b[0m )\n\u001b[0;32m   4535\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m: train_split, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m: test_split})\n","File \u001b[1;32mc:\\Users\\hzf04d\\Anaconda3\\envs\\llm2\\Lib\\site-packages\\datasets\\arrow_dataset.py:557\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    550\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    551\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[0;32m    555\u001b[0m }\n\u001b[0;32m    556\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 557\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    558\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    559\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\hzf04d\\Anaconda3\\envs\\llm2\\Lib\\site-packages\\datasets\\fingerprint.py:481\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    477\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[0;32m    479\u001b[0m \u001b[38;5;66;03m# Call actual function\u001b[39;00m\n\u001b[1;32m--> 481\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:  \u001b[38;5;66;03m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\hzf04d\\Anaconda3\\envs\\llm2\\Lib\\site-packages\\datasets\\arrow_dataset.py:3808\u001b[0m, in \u001b[0;36mDataset.select\u001b[1;34m(self, indices, keep_in_memory, indices_cache_file_name, writer_batch_size, new_fingerprint)\u001b[0m\n\u001b[0;32m   3805\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_contiguous(start, length, new_fingerprint\u001b[38;5;241m=\u001b[39mnew_fingerprint)\n\u001b[0;32m   3807\u001b[0m \u001b[38;5;66;03m# If not contiguous, we need to create a new indices mapping\u001b[39;00m\n\u001b[1;32m-> 3808\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_select_with_indices_mapping\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3809\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3810\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3811\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindices_cache_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindices_cache_file_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3812\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3813\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_fingerprint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3814\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\hzf04d\\Anaconda3\\envs\\llm2\\Lib\\site-packages\\datasets\\arrow_dataset.py:557\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    550\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    551\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[0;32m    555\u001b[0m }\n\u001b[0;32m    556\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 557\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    558\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    559\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\hzf04d\\Anaconda3\\envs\\llm2\\Lib\\site-packages\\datasets\\fingerprint.py:481\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    477\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[0;32m    479\u001b[0m \u001b[38;5;66;03m# Call actual function\u001b[39;00m\n\u001b[1;32m--> 481\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:  \u001b[38;5;66;03m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\hzf04d\\Anaconda3\\envs\\llm2\\Lib\\site-packages\\datasets\\arrow_dataset.py:3964\u001b[0m, in \u001b[0;36mDataset._select_with_indices_mapping\u001b[1;34m(self, indices, keep_in_memory, indices_cache_file_name, writer_batch_size, new_fingerprint)\u001b[0m\n\u001b[0;32m   3962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tmp_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3963\u001b[0m     tmp_file\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m-> 3964\u001b[0m     \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmove\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmp_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices_cache_file_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3965\u001b[0m     umask \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mumask(\u001b[38;5;241m0o666\u001b[39m)\n\u001b[0;32m   3966\u001b[0m     os\u001b[38;5;241m.\u001b[39mumask(umask)\n","File \u001b[1;32mc:\\Users\\hzf04d\\Anaconda3\\envs\\llm2\\Lib\\shutil.py:906\u001b[0m, in \u001b[0;36mmove\u001b[1;34m(src, dst, copy_function)\u001b[0m\n\u001b[0;32m    904\u001b[0m         rmtree(src)\n\u001b[0;32m    905\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 906\u001b[0m         \u001b[43mcopy_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_dst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    907\u001b[0m         os\u001b[38;5;241m.\u001b[39munlink(src)\n\u001b[0;32m    908\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m real_dst\n","File \u001b[1;32mc:\\Users\\hzf04d\\Anaconda3\\envs\\llm2\\Lib\\shutil.py:460\u001b[0m, in \u001b[0;36mcopy2\u001b[1;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[0;32m    458\u001b[0m     flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m _winapi\u001b[38;5;241m.\u001b[39mCOPY_FILE_COPY_SYMLINK\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 460\u001b[0m     \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCopyFile2\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    461\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dst\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n","\u001b[1;31mOSError\u001b[0m: [WinError 1224] L’opération demandée n’a pu s’accomplir sur un fichier ayant une section mappée utilisateur ouverte"]}],"source":["# Create training and evaluation datasets\n","if sample_size < 0:\n","    train_sample = round(raw_dataset['train'].num_rows * train_ratio)\n","    test_sample = round(raw_dataset['train'].num_rows * (1- train_ratio))\n","else:\n","    train_sample = round(sample_size * train_ratio)\n","    test_sample = round(sample_size * (1- train_ratio))\n","\n","ds_train_test = raw_dataset['train'].train_test_split(train_size=train_sample, test_size=test_sample, seed=SEED)\n","train_text = ''.join([t['text'] for t in ds_train_test['train']])"]},{"cell_type":"markdown","metadata":{},"source":["# Step 1\n","Create a simple Byte Pair Encoder (BPE)"]},{"cell_type":"code","execution_count":97,"metadata":{},"outputs":[],"source":["from collections import Counter, OrderedDict\n","\n","class BasicTokenizer():\n","    def __init__(self):\n","        self.text_to_int = {}  # {text: int}\n","        self.int_to_text = {}  # {int: text}\n","        self.merged_dict = OrderedDict()  # {(token, token): token}\n","        \n","    def train(self, text, max_vocab_size, verbose=False):\n","        self.text_to_int, self.int_to_text = self._create_initial_vocabulary(text)  \n","        self.current_vocab_size = len(self.text_to_int)\n","        \n","        train_tokens = self.encode(text) \n","        \n","        if verbose: \n","            self.print_stats(train_tokens)\n","        \n","        while self.current_vocab_size < max_vocab_size:\n","            token_pairs = self._get_pairs(train_tokens)\n","            most_freq = self._get_most_common_token_pair(token_pairs)\n","            \n","            str_pair = ''.join([self.int_to_text[most_freq[0]], self.int_to_text[most_freq[1]]])\n","            self.text_to_int[str_pair] = self.current_vocab_size\n","            self.int_to_text[self.current_vocab_size] = str_pair\n","            self.merged_dict[most_freq] = self.text_to_int[str_pair]\n","            self.current_vocab_size += 1\n","\n","            new_train_tokens = self._replace_token_pairs(train_tokens, most_freq, self.text_to_int[str_pair])\n","\n","            train_tokens = new_train_tokens\n","\n","            if verbose:\n","                self.print_stats(train_tokens, str_pair, most_freq)\n","\n","    def print_stats(self, train_tokens, str_pair=\"\", token_pair=None):\n","        print(\"Updated vocab size:\", self.current_vocab_size, \n","              \"Updated sequence length:\", len(train_tokens), \n","              \"New vocab:\", str_pair, \n","              \"Token pair:\", token_pair)\n","        \n","    def _get_pairs(self, tokens):\n","        return list(zip(tokens, tokens[1:]))\n","    \n","    def _create_initial_vocabulary(self, text):\n","        sorted_text = sorted(set(text))\n","        text_to_int = {c: i for i, c in enumerate(sorted_text)}\n","        int_to_text = {i: c for i, c in enumerate(sorted_text)}\n","        return text_to_int, int_to_text\n","\n","    def _get_most_common_token_pair(self, token_pairs):\n","        counter = Counter(token_pairs)\n","        most_freq = counter.most_common(1)[0][0]\n","        return most_freq\n","    \n","    def _replace_token_pairs(self, tokens, target_pair, new_token):\n","        \"\"\"\n","        Replace all occurrences of `target_pair` in `tokens` by `new_token`.\n","        \"\"\"\n","        new_tokens = []\n","        i = 0\n","        while i < len(tokens) - 1:\n","            if (tokens[i], tokens[i+1]) == target_pair:\n","                new_tokens.append(new_token)\n","                i += 2\n","            else:\n","                new_tokens.append(tokens[i])\n","                i += 1\n","        if i < len(tokens):\n","            new_tokens.append(tokens[i])\n","        return new_tokens\n","\n","    def encode(self, text):\n","        tokens = [self.text_to_int[char] for char in text]\n","        for key in self.merged_dict:\n","            tokens = self._replace_token_pairs(tokens, key, self.merged_dict[key])\n","        return tokens\n","            \n","    \n","    def decode(self, tokens):\n","        return \"\".join([self.int_to_text[i] for i in tokens])\n"]},{"cell_type":"code","execution_count":98,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Updated vocab size: 94 Updated sequence length: 35611 New vocab:  Token pair: None\n","Updated vocab size: 95 Updated sequence length: 34076 New vocab: e  Token pair: (54, 2)\n","Updated vocab size: 96 Updated sequence length: 33269 New vocab: s  Token pair: (68, 2)\n","Updated vocab size: 97 Updated sequence length: 32752 New vocab: on Token pair: (64, 63)\n","Updated vocab size: 98 Updated sequence length: 32259 New vocab: t  Token pair: (69, 2)\n","Updated vocab size: 99 Updated sequence length: 31824 New vocab: de  Token pair: (53, 94)\n","Updated vocab size: 100 Updated sequence length: 31391 New vocab: an Token pair: (50, 63)\n","Updated vocab size: 101 Updated sequence length: 30960 New vocab: en Token pair: (54, 63)\n","Updated vocab size: 102 Updated sequence length: 30598 New vocab: es  Token pair: (54, 95)\n","Updated vocab size: 103 Updated sequence length: 30280 New vocab: ti Token pair: (69, 58)\n","Updated vocab size: 104 Updated sequence length: 29982 New vocab: ,  Token pair: (6, 2)\n","Updated vocab size: 105 Updated sequence length: 29686 New vocab: in Token pair: (58, 63)\n","Updated vocab size: 106 Updated sequence length: 29398 New vocab: er Token pair: (54, 67)\n","Updated vocab size: 107 Updated sequence length: 29112 New vocab: a  Token pair: (50, 2)\n","Updated vocab size: 108 Updated sequence length: 28856 New vocab: qu Token pair: (66, 70)\n","Updated vocab size: 109 Updated sequence length: 28617 New vocab: ur Token pair: (70, 67)\n","Updated vocab size: 110 Updated sequence length: 28388 New vocab: un Token pair: (70, 63)\n","Updated vocab size: 111 Updated sequence length: 28184 New vocab: tr Token pair: (69, 67)\n","Updated vocab size: 112 Updated sequence length: 27982 New vocab: la  Token pair: (61, 106)\n","Updated vocab size: 113 Updated sequence length: 27781 New vocab: le  Token pair: (61, 94)\n","Updated vocab size: 114 Updated sequence length: 27582 New vocab: ar Token pair: (50, 67)\n","Updated vocab size: 115 Updated sequence length: 27394 New vocab:  d Token pair: (2, 53)\n","Updated vocab size: 116 Updated sequence length: 27207 New vocab: es Token pair: (54, 68)\n","Updated vocab size: 117 Updated sequence length: 27020 New vocab: is Token pair: (58, 68)\n","Updated vocab size: 118 Updated sequence length: 26847 New vocab: e de  Token pair: (94, 98)\n","Updated vocab size: 119 Updated sequence length: 26682 New vocab: or Token pair: (64, 67)\n","Updated vocab size: 120 Updated sequence length: 26532 New vocab: om Token pair: (64, 62)\n","Updated vocab size: 121 Updated sequence length: 26389 New vocab: el Token pair: (54, 61)\n","Updated vocab size: 122 Updated sequence length: 26248 New vocab: on  Token pair: (96, 2)\n","Updated vocab size: 123 Updated sequence length: 26110 New vocab: pr Token pair: (65, 67)\n","Updated vocab size: 124 Updated sequence length: 25976 New vocab: ec Token pair: (54, 52)\n","Updated vocab size: 125 Updated sequence length: 25846 New vocab: si Token pair: (68, 58)\n","Updated vocab size: 126 Updated sequence length: 25717 New vocab: é  Token pair: (86, 2)\n","Updated vocab size: 127 Updated sequence length: 25588 New vocab: at Token pair: (50, 69)\n","Updated vocab size: 128 Updated sequence length: 25460 New vocab: al Token pair: (50, 61)\n","Updated vocab size: 129 Updated sequence length: 25335 New vocab: au Token pair: (50, 70)\n","Updated vocab size: 130 Updated sequence length: 25217 New vocab: em Token pair: (54, 62)\n","Updated vocab size: 131 Updated sequence length: 25099 New vocab: \n","\n"," Token pair: (1, 1)\n","Updated vocab size: 132 Updated sequence length: 24983 New vocab: et  Token pair: (54, 97)\n","Updated vocab size: 133 Updated sequence length: 24867 New vocab: ai Token pair: (50, 58)\n","Updated vocab size: 134 Updated sequence length: 24755 New vocab: \n","  Token pair: (1, 2)\n","Updated vocab size: 135 Updated sequence length: 24645 New vocab: ou Token pair: (64, 70)\n","Updated vocab size: 136 Updated sequence length: 24535 New vocab: ent  Token pair: (100, 97)\n","Updated vocab size: 137 Updated sequence length: 24425 New vocab: il Token pair: (58, 61)\n","Updated vocab size: 138 Updated sequence length: 24316 New vocab: ans  Token pair: (99, 95)\n","Updated vocab size: 139 Updated sequence length: 24210 New vocab: en  Token pair: (100, 2)\n","Updated vocab size: 140 Updated sequence length: 24106 New vocab: e d Token pair: (94, 53)\n","Updated vocab size: 141 Updated sequence length: 24003 New vocab: ati Token pair: (50, 102)\n","Updated vocab size: 142 Updated sequence length: 23901 New vocab: l' Token pair: (61, 3)\n","Updated vocab size: 143 Updated sequence length: 23803 New vocab: it Token pair: (58, 69)\n","Updated vocab size: 144 Updated sequence length: 23708 New vocab: ch Token pair: (52, 57)\n","Updated vocab size: 145 Updated sequence length: 23613 New vocab: à  Token pair: (82, 2)\n","Updated vocab size: 146 Updated sequence length: 23519 New vocab: .  Token pair: (8, 2)\n","Updated vocab size: 147 Updated sequence length: 23425 New vocab: ér Token pair: (86, 67)\n","Updated vocab size: 148 Updated sequence length: 23340 New vocab: ur  Token pair: (108, 2)\n","Updated vocab size: 149 Updated sequence length: 23255 New vocab: les  Token pair: (61, 101)\n","Updated vocab size: 150 Updated sequence length: 23172 New vocab: lo Token pair: (61, 64)\n","len original text: 40, len bpe text: 26\n","[25, 96, 59, 64, 147, 112, 62, 96, 53, 54, 103, 52, 119, 62, 135, 71, 106, 69, 106, 59, 64, 108, 63, 86, 54, 23]\n","Bonjour le monde, comment va ta journée?\n"]}],"source":["tokenizer = BasicTokenizer()\n","tokenizer.train(train_text, max_vocab_size=150, verbose=True)\n","\n","text = \"Bonjour le monde, comment va ta journée?\"\n","code = tokenizer.encode(text)\n","print(f\"len original text: {len(text)}, len bpe text: {len(code)}\") \n","print(code)\n","print(tokenizer.decode(code))"]},{"cell_type":"markdown","metadata":{},"source":["# Step 2\n","Improve the BPE from step 1 by splitting first the text into words and then applying the BPE to each word. This will avoid having tokens that are composed of multiple words or heterogeneous (word + number for example)"]},{"cell_type":"code","execution_count":99,"metadata":{},"outputs":[],"source":["import regex as re\n","\n","class RegexTokenizer():\n","    def __init__(self):\n","        self.text_to_int = {}  # {text: int}\n","        self.int_to_text = {}  # {int: text}\n","        self.merged_dict = OrderedDict()  # {(token, token): token}\n","        \n","    def train(self, text, max_vocab_size, verbose=False):\n","        self.text_to_int, self.int_to_text = self._create_initial_vocabulary(text)\n","        self.current_vocab_size = len(self.text_to_int)\n","        \n","        tokens_splits = self.encode(text)\n","        \n","        if verbose: \n","            self.print_stats(tokens_splits)\n","            \n","        while self.current_vocab_size < max_vocab_size:\n","            token_pairs = []\n","            for tokens_split in tokens_splits:\n","                if len(tokens_split) > 1:\n","                    token_pairs.extend(self._get_pairs(tokens_split))\n","                    \n","            most_freq = self._get_most_common_token_pair(token_pairs)\n","            \n","            str_pair = ''.join([self.int_to_text[most_freq[0]], self.int_to_text[most_freq[1]]])\n","            self.text_to_int[str_pair] = self.current_vocab_size\n","            self.int_to_text[self.current_vocab_size] = str_pair\n","            self.merged_dict[most_freq] = self.text_to_int[str_pair]\n","            \n","            new_tokens_splits = [self._replace_token_pairs(t, most_freq, self.current_vocab_size) for t in tokens_splits]\n","            tokens_splits = new_tokens_splits\n","            self.current_vocab_size += 1\n","            \n","            if verbose: \n","                self.print_stats(tokens_splits, str_pair, most_freq)\n","\n","    def print_stats(self, tokens_splits, str_pair=\"\", token_pair=None):\n","        print(\"Updated vocab size:\", self.current_vocab_size, \n","              \"Updated sequence length:\", sum([len(split) for split in tokens_splits]),\n","              \"New vocab:\", str_pair, \n","              \"Token pair:\", token_pair)\n","        \n","    def _get_pairs(self, tokens):\n","        return list(zip(tokens, tokens[1:]))\n","    \n","    def _create_initial_vocabulary(self, text):\n","        sorted_text = sorted(set(text))\n","        text_to_int = {c: i for i, c in enumerate(sorted_text)}\n","        int_to_text = {i: c for i, c in enumerate(sorted_text)}\n","        return text_to_int, int_to_text\n","\n","    def _get_most_common_token_pair(self, token_pairs):\n","        counter = Counter(token_pairs)\n","        most_freq = counter.most_common(1)[0][0]\n","        return most_freq\n","    \n","    def _replace_token_pairs(self, tokens, target_pair, new_token):\n","        \"\"\"\n","        Replace all occurrences of `target_pair` in `tokens` by `new_token`.\n","        \"\"\"\n","        new_tokens = []\n","        i = 0\n","        while i < len(tokens) - 1:\n","            if (tokens[i], tokens[i+1]) == target_pair:\n","                new_tokens.append(new_token)\n","                i += 2\n","            else:\n","                new_tokens.append(tokens[i])\n","                i += 1\n","        if i < len(tokens):\n","            new_tokens.append(tokens[i])\n","        return new_tokens\n","\n","    def _simple_encode(self, text):\n","        \n","        tokens = [self.text_to_int[char] for char in text]\n","        for key in self.merged_dict:\n","            tokens = self._replace_token_pairs(tokens, key, self.merged_dict[key])\n","        return tokens\n","    \n","    def encode(self, text):\n","        text_splits = self._split_text(text)\n","        tokens_splits = [self._simple_encode(text_split) for text_split in text_splits]\n","        return tokens_splits\n","        \n","    def _split_text(self, text):\n","        split_text = re.findall(GPT4_SPLIT_PATTERN, text)\n","        return split_text\n","    \n","    def decode(self, tokens_splits):\n","        texts = []\n","        for tokens in tokens_splits:\n","            texts.append(\"\".join([self.int_to_text[i] for i in tokens]))\n","        return \"\".join(texts)\n"]},{"cell_type":"code","execution_count":102,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Updated vocab size: 94 Updated sequence length: 35611 New vocab:  Token pair: None\n","Updated vocab size: 95 Updated sequence length: 34713 New vocab:  d Token pair: (2, 53)\n","Updated vocab size: 96 Updated sequence length: 34164 New vocab: es Token pair: (54, 68)\n","Updated vocab size: 97 Updated sequence length: 33632 New vocab:  l Token pair: (2, 61)\n","Updated vocab size: 98 Updated sequence length: 33115 New vocab: on Token pair: (64, 63)\n","Updated vocab size: 99 Updated sequence length: 32676 New vocab:  de Token pair: (94, 54)\n","Updated vocab size: 100 Updated sequence length: 32243 New vocab: an Token pair: (50, 63)\n","Updated vocab size: 101 Updated sequence length: 31812 New vocab: en Token pair: (54, 63)\n","Updated vocab size: 102 Updated sequence length: 31451 New vocab:  p Token pair: (2, 65)\n","Updated vocab size: 103 Updated sequence length: 31133 New vocab: ti Token pair: (69, 58)\n","Updated vocab size: 104 Updated sequence length: 30823 New vocab:  c Token pair: (2, 52)\n","Updated vocab size: 105 Updated sequence length: 30527 New vocab: in Token pair: (58, 63)\n","Updated vocab size: 106 Updated sequence length: 30242 New vocab: er Token pair: (54, 67)\n","Updated vocab size: 107 Updated sequence length: 29985 New vocab:  s Token pair: (2, 68)\n","Updated vocab size: 108 Updated sequence length: 29729 New vocab: qu Token pair: (66, 70)\n","Updated vocab size: 109 Updated sequence length: 29490 New vocab: ur Token pair: (70, 67)\n","Updated vocab size: 110 Updated sequence length: 29258 New vocab: is Token pair: (58, 68)\n","Updated vocab size: 111 Updated sequence length: 29029 New vocab: un Token pair: (70, 63)\n","Updated vocab size: 112 Updated sequence length: 28817 New vocab: re Token pair: (67, 54)\n","Updated vocab size: 113 Updated sequence length: 28612 New vocab:  a Token pair: (2, 50)\n","Updated vocab size: 114 Updated sequence length: 28413 New vocab:  la Token pair: (96, 50)\n","Updated vocab size: 115 Updated sequence length: 28230 New vocab: ar Token pair: (50, 67)\n","Updated vocab size: 116 Updated sequence length: 28048 New vocab: le Token pair: (61, 54)\n","Updated vocab size: 117 Updated sequence length: 27870 New vocab: te Token pair: (69, 54)\n","Updated vocab size: 118 Updated sequence length: 27710 New vocab: ent Token pair: (100, 69)\n","Updated vocab size: 119 Updated sequence length: 27555 New vocab: or Token pair: (64, 67)\n","Updated vocab size: 120 Updated sequence length: 27405 New vocab: om Token pair: (64, 62)\n","Updated vocab size: 121 Updated sequence length: 27256 New vocab:  e Token pair: (2, 54)\n","Updated vocab size: 122 Updated sequence length: 27110 New vocab: tr Token pair: (69, 67)\n","Updated vocab size: 123 Updated sequence length: 26974 New vocab: tion Token pair: (102, 97)\n","Updated vocab size: 124 Updated sequence length: 26839 New vocab: el Token pair: (54, 61)\n","Updated vocab size: 125 Updated sequence length: 26706 New vocab: it Token pair: (58, 69)\n","Updated vocab size: 126 Updated sequence length: 26577 New vocab:  un Token pair: (2, 110)\n","Updated vocab size: 127 Updated sequence length: 26450 New vocab:  C Token pair: (2, 26)\n","Updated vocab size: 128 Updated sequence length: 26324 New vocab: que Token pair: (107, 54)\n","Updated vocab size: 129 Updated sequence length: 26200 New vocab: ans Token pair: (99, 68)\n","Updated vocab size: 130 Updated sequence length: 26077 New vocab:  le Token pair: (96, 54)\n","Updated vocab size: 131 Updated sequence length: 25955 New vocab: il Token pair: (58, 61)\n","Updated vocab size: 132 Updated sequence length: 25833 New vocab: ce Token pair: (52, 54)\n","Updated vocab size: 133 Updated sequence length: 25715 New vocab: \n","\n"," Token pair: (1, 1)\n","Updated vocab size: 134 Updated sequence length: 25604 New vocab:  f Token pair: (2, 55)\n","Updated vocab size: 135 Updated sequence length: 25494 New vocab: ou Token pair: (64, 70)\n","Updated vocab size: 136 Updated sequence length: 25386 New vocab:  en Token pair: (2, 100)\n","Updated vocab size: 137 Updated sequence length: 25280 New vocab: al Token pair: (50, 61)\n","Updated vocab size: 138 Updated sequence length: 25174 New vocab:  pr Token pair: (101, 67)\n","Updated vocab size: 139 Updated sequence length: 25070 New vocab: at Token pair: (50, 69)\n","Updated vocab size: 140 Updated sequence length: 24967 New vocab: si Token pair: (68, 58)\n","Updated vocab size: 141 Updated sequence length: 24865 New vocab:  n Token pair: (2, 63)\n","Updated vocab size: 142 Updated sequence length: 24763 New vocab:  m Token pair: (2, 62)\n","Updated vocab size: 143 Updated sequence length: 24664 New vocab: ment Token pair: (62, 117)\n","Updated vocab size: 144 Updated sequence length: 24565 New vocab:  dans Token pair: (94, 128)\n","Updated vocab size: 145 Updated sequence length: 24471 New vocab:  à Token pair: (2, 82)\n","Updated vocab size: 146 Updated sequence length: 24377 New vocab: ér Token pair: (86, 67)\n","Updated vocab size: 147 Updated sequence length: 24286 New vocab:  et Token pair: (120, 69)\n","Updated vocab size: 148 Updated sequence length: 24196 New vocab:  é Token pair: (2, 86)\n","Updated vocab size: 149 Updated sequence length: 24107 New vocab: ie Token pair: (58, 54)\n","Updated vocab size: 150 Updated sequence length: 24020 New vocab: me Token pair: (62, 54)\n","len original text: 40, len bpe text: 28\n","[[25, 97, 59, 64, 108], [129], [141, 97, 53, 54], [6], [103, 119, 142], [2, 71, 50], [2, 69, 50], [2, 59, 64, 108, 63, 86, 54], [23]]\n","Bonjour le monde, comment va ta journée?\n"]}],"source":["GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n","tokenizer = RegexTokenizer()\n","tokenizer.train(train_text, max_vocab_size=150, verbose=True)\n","\n","text = \"Bonjour le monde, comment va ta journée?\"\n","code = tokenizer.encode(text)\n","print(f\"len original text: {len(text)}, len bpe text: {sum([len(c) for c in code])}\")\n","print(code)\n","print(tokenizer.decode(code))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":3521629,"sourceId":6146260,"sourceType":"datasetVersion"}],"dockerImageVersionId":30648,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":4}
