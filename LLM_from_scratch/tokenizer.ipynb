{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Sources:\n","1 - https://karpathy.ai/zero-to-hero.html  \n","2 - https://www.youtube.com/watch?v=kCc8FmEb1nY  \n","3 - https://github.com/karpathy/minbpe/tree/master  "]},{"cell_type":"markdown","metadata":{},"source":["# Import libraries"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T09:25:01.248595Z","iopub.status.busy":"2024-02-26T09:25:01.248278Z","iopub.status.idle":"2024-02-26T09:25:01.256637Z","shell.execute_reply":"2024-02-26T09:25:01.255806Z","shell.execute_reply.started":"2024-02-26T09:25:01.248567Z"},"trusted":true},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn as nn\n","from torch.nn import functional as F\n","from tqdm import tqdm\n","import time"]},{"cell_type":"markdown","metadata":{},"source":["# Parameters"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T09:25:01.258001Z","iopub.status.busy":"2024-02-26T09:25:01.257729Z","iopub.status.idle":"2024-02-26T09:25:01.267343Z","shell.execute_reply":"2024-02-26T09:25:01.266449Z","shell.execute_reply.started":"2024-02-26T09:25:01.257979Z"},"trusted":true},"outputs":[],"source":["# Dataset parameters\n","train_ratio = 0.9\n","sample_size = 10\n","\n","SEED = 15"]},{"cell_type":"markdown","metadata":{},"source":["# Load files"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T09:25:01.296859Z","iopub.status.busy":"2024-02-26T09:25:01.296541Z","iopub.status.idle":"2024-02-26T09:25:09.594297Z","shell.execute_reply":"2024-02-26T09:25:09.593329Z","shell.execute_reply.started":"2024-02-26T09:25:01.296836Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\hzf04d\\Anaconda3\\envs\\llm2\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","Resolving data files: 100%|██████████| 17/17 [00:01<00:00,  8.75it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Dataset size: 2564646\n"]}],"source":["from datasets import load_dataset\n","\n","# Define the path to the dataset\n","dataset_name = \"20231101.fr\"\n","\n","# Load the dataset\n","raw_dataset = load_dataset(\"wikimedia/wikipedia\", dataset_name)\n","print(f\"Dataset size: {raw_dataset['train'].num_rows}\")"]},{"cell_type":"markdown","metadata":{},"source":["# Data preparation"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# Create training and evaluation datasets\n","if sample_size < 0:\n","    train_sample = round(raw_dataset['train'].num_rows * train_ratio)\n","    test_sample = round(raw_dataset['train'].num_rows * (1- train_ratio))\n","else:\n","    train_sample = round(sample_size * train_ratio)\n","    test_sample = round(sample_size * (1- train_ratio))\n","\n","ds_train_test = raw_dataset['train'].train_test_split(train_size=train_sample, test_size=test_sample, seed=SEED)\n","train_text = ''.join([t['text'] for t in ds_train_test['train']])"]},{"cell_type":"markdown","metadata":{},"source":["# Step 1"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["from collections import Counter\n","\n","class BasicTokenizer():\n","    def __init__(self):\n","        self.text_to_int = {}\n","        self.int_to_text = {}\n","        \n","    def train(self, text, max_vocab_size, verbose=False):\n","        self.text_to_int, self.int_to_text = self._create_init_vocab(text)\n","        train_tokens = self.encode(text)   \n","        current_vocab_size = len(self.text_to_int)\n","        \n","        if verbose: \n","            print(\"Initial vocab size:\", current_vocab_size, \"Initial sequence length:\", len(train_tokens))\n","        \n","        while current_vocab_size < max_vocab_size:\n","            token_pairs = list(zip(train_tokens, train_tokens[1:]))\n","            most_freq = self._get_most_common_token_pair(token_pairs)\n","            \n","            str_pair = ''.join([self.int_to_text[most_freq[0]], self.int_to_text[most_freq[1]]])\n","            self.text_to_int[str_pair] = current_vocab_size\n","            self.int_to_text[current_vocab_size] = str_pair\n","            current_vocab_size += 1\n","\n","            new_train_tokens = self._replace_token_pairs(train_tokens, most_freq, self.text_to_int[str_pair])\n","\n","            train_tokens = new_train_tokens\n","\n","            if verbose:\n","                print(\"Updated vocab size:\", current_vocab_size, \"Updated sequence length:\", len(train_tokens))\n","\n","    def _create_init_vocab(self, text):\n","        sorted_text = sorted(set(text))\n","        text_to_int = {c: i for i, c in enumerate(sorted_text)}\n","        int_to_text = {i: c for i, c in enumerate(sorted_text)}\n","        return text_to_int, int_to_text\n","\n","    def _get_most_common_token_pair(self, token_pairs):\n","        counter = Counter(token_pairs)\n","        most_freq = counter.most_common(1)[0][0]\n","        return most_freq\n","    \n","    def _replace_token_pairs(self, tokens, target_pair, new_token):\n","        new_tokens = []\n","        i = 0\n","        while i < len(tokens) - 1:\n","            if (tokens[i], tokens[i+1]) == target_pair:\n","                new_tokens.append(new_token)\n","                i += 2\n","            else:\n","                new_tokens.append(tokens[i])\n","                i += 1\n","        if i < len(tokens):\n","            new_tokens.append(tokens[i])\n","        return new_tokens\n","\n","    def encode(self, text):\n","        return [self.text_to_int[char] for char in text]\n","    \n","    def decode(self, tokens):\n","        return \"\".join([self.int_to_text[i] for i in tokens])\n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["tokenizer = BasicTokenizer()\n","tokenizer.train(train_text, max_vocab_size=150, verbose=True)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/plain":["[25, 64, 63, 59, 64, 70, 67]"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.encode('Bonjour')"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"data":{"text/plain":["'Bonjour'"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.decode([25, 64, 63, 59, 64, 70, 67])"]},{"cell_type":"markdown","metadata":{},"source":["# Step 2"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[],"source":["from collections import Counter\n","import regex as re\n","\n","class RegexTokenizer():\n","    def __init__(self):\n","        self.text_to_int = {}\n","        self.int_to_text = {}\n","        \n","    def train(self, text, max_vocab_size, verbose=False):\n","        self.text_to_int, self.int_to_text = self._create_init_vocab(text)\n","        split_tokens = self.encode(text)\n","        self.current_vocab_size = len(self.text_to_int)\n","        \n","        if verbose:\n","            self.print_stats(split_tokens)\n","            \n","        while self.current_vocab_size < max_vocab_size:\n","            token_pairs = []\n","            for t in split_tokens:\n","                if len(t) > 1:\n","                    token_pairs.extend(self._get_token_pairs(t))\n","\n","            most_freq = self._get_most_common_token_pair(token_pairs)\n","            most_freq_str = ''.join([self.int_to_text[most_freq[0]], self.int_to_text[most_freq[1]]])\n","            \n","            self.text_to_int[most_freq_str] = self.current_vocab_size\n","            self.int_to_text[self.current_vocab_size] = most_freq_str\n","            \n","            new_split_tokens = [self._replace_token_pairs(t, most_freq, self.current_vocab_size) for t in split_tokens]\n","            split_tokens = new_split_tokens\n","            self.current_vocab_size += 1\n","            \n","            if verbose:\n","                self.print_stats(split_tokens, most_freq_str)\n","\n","    def print_stats(self, split_tokens, new_token = \"\"):\n","        print(\"Initial vocab size:\", self.current_vocab_size, \"Initial sequence length:\", sum([len(t) for t in split_tokens]), \"New token:\", new_token)\n","        \n","    def _get_token_pairs(self, tokens):\n","        return list(zip(tokens, tokens[1:]))\n","    \n","    def _split_text(self, text):\n","        split_text = re.findall(GPT4_SPLIT_PATTERN, text)\n","        return split_text\n","    \n","    def _create_init_vocab(self, text):\n","        sorted_text = sorted(set(text))\n","        text_to_int = {c: i for i, c in enumerate(sorted_text)}\n","        int_to_text = {i: c for i, c in enumerate(sorted_text)}\n","        return text_to_int, int_to_text\n","\n","    def _get_most_common_token_pair(self, token_pairs):\n","        counter = Counter(token_pairs)\n","        most_freq = counter.most_common(1)[0][0]\n","        return most_freq\n","    \n","    def _replace_token_pairs(self, tokens, target_pair, new_token):\n","        new_tokens = []\n","        i = 0\n","        while i < len(tokens) - 1:\n","            if (tokens[i], tokens[i+1]) == target_pair:\n","                new_tokens.append(new_token)\n","                i += 2\n","            else:\n","                new_tokens.append(tokens[i])\n","                i += 1\n","        if i < len(tokens):\n","            new_tokens.append(tokens[i])\n","        return new_tokens\n","\n","    def encode(self, text):\n","        split_text = self._split_text(text)\n","        encoded_splits = []\n","        for split in split_text:\n","            encoded_splits.append([self.text_to_int[t] for t in split])\n","        return encoded_splits\n","    \n","    def decode(self, tokens_list):\n","        texts = []\n","        for tokens in tokens_list:\n","            texts.append(\"\".join([self.int_to_text[i] for i in tokens]))\n","        return \"\".join(texts)\n"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Initial vocab size: 94 Initial sequence length: 35611 New token: \n","Initial vocab size: 95 Initial sequence length: 34713 New token:  d\n","Initial vocab size: 96 Initial sequence length: 34164 New token: es\n","Initial vocab size: 97 Initial sequence length: 33632 New token:  l\n","Initial vocab size: 98 Initial sequence length: 33115 New token: on\n","Initial vocab size: 99 Initial sequence length: 32676 New token:  de\n","Initial vocab size: 100 Initial sequence length: 32243 New token: an\n","Initial vocab size: 101 Initial sequence length: 31812 New token: en\n","Initial vocab size: 102 Initial sequence length: 31451 New token:  p\n","Initial vocab size: 103 Initial sequence length: 31133 New token: ti\n","Initial vocab size: 104 Initial sequence length: 30823 New token:  c\n","Initial vocab size: 105 Initial sequence length: 30527 New token: in\n","Initial vocab size: 106 Initial sequence length: 30242 New token: er\n","Initial vocab size: 107 Initial sequence length: 29985 New token:  s\n","Initial vocab size: 108 Initial sequence length: 29729 New token: qu\n","Initial vocab size: 109 Initial sequence length: 29490 New token: ur\n","Initial vocab size: 110 Initial sequence length: 29258 New token: is\n","Initial vocab size: 111 Initial sequence length: 29029 New token: un\n","Initial vocab size: 112 Initial sequence length: 28817 New token: re\n","Initial vocab size: 113 Initial sequence length: 28612 New token:  a\n","Initial vocab size: 114 Initial sequence length: 28413 New token:  la\n","Initial vocab size: 115 Initial sequence length: 28230 New token: ar\n","Initial vocab size: 116 Initial sequence length: 28048 New token: le\n","Initial vocab size: 117 Initial sequence length: 27870 New token: te\n","Initial vocab size: 118 Initial sequence length: 27710 New token: ent\n","Initial vocab size: 119 Initial sequence length: 27555 New token: or\n","Initial vocab size: 120 Initial sequence length: 27405 New token: om\n","Initial vocab size: 121 Initial sequence length: 27256 New token:  e\n","Initial vocab size: 122 Initial sequence length: 27110 New token: tr\n","Initial vocab size: 123 Initial sequence length: 26974 New token: tion\n","Initial vocab size: 124 Initial sequence length: 26839 New token: el\n","Initial vocab size: 125 Initial sequence length: 26706 New token: it\n","Initial vocab size: 126 Initial sequence length: 26577 New token:  un\n","Initial vocab size: 127 Initial sequence length: 26450 New token:  C\n","Initial vocab size: 128 Initial sequence length: 26324 New token: que\n","Initial vocab size: 129 Initial sequence length: 26200 New token: ans\n","Initial vocab size: 130 Initial sequence length: 26077 New token:  le\n","Initial vocab size: 131 Initial sequence length: 25955 New token: il\n","Initial vocab size: 132 Initial sequence length: 25833 New token: ce\n","Initial vocab size: 133 Initial sequence length: 25715 New token: \n","\n","\n","Initial vocab size: 134 Initial sequence length: 25604 New token:  f\n","Initial vocab size: 135 Initial sequence length: 25494 New token: ou\n","Initial vocab size: 136 Initial sequence length: 25386 New token:  en\n","Initial vocab size: 137 Initial sequence length: 25280 New token: al\n","Initial vocab size: 138 Initial sequence length: 25174 New token:  pr\n","Initial vocab size: 139 Initial sequence length: 25070 New token: at\n","Initial vocab size: 140 Initial sequence length: 24967 New token: si\n","Initial vocab size: 141 Initial sequence length: 24865 New token:  n\n","Initial vocab size: 142 Initial sequence length: 24763 New token:  m\n","Initial vocab size: 143 Initial sequence length: 24664 New token: ment\n","Initial vocab size: 144 Initial sequence length: 24565 New token:  dans\n","Initial vocab size: 145 Initial sequence length: 24471 New token:  à\n","Initial vocab size: 146 Initial sequence length: 24377 New token: ér\n","Initial vocab size: 147 Initial sequence length: 24286 New token:  et\n","Initial vocab size: 148 Initial sequence length: 24196 New token:  é\n","Initial vocab size: 149 Initial sequence length: 24107 New token: ie\n","Initial vocab size: 150 Initial sequence length: 24020 New token: me\n"]}],"source":["GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n","tokenizer = RegexTokenizer()\n","tokenizer.train(train_text, max_vocab_size=150, verbose=True)"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[{"data":{"text/plain":["[[25, 64, 63, 59, 64, 70, 67], [2, 61, 54], [2, 62, 64, 63, 53, 54]]"]},"execution_count":55,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.encode(\"Bonjour le monde\")"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[{"data":{"text/plain":["'Bonjour le monde'"]},"execution_count":56,"metadata":{},"output_type":"execute_result"}],"source":["generated_ids = [[25, 64, 63, 59, 64, 70, 67], [2, 61, 54], [2, 62, 64, 63, 53, 54]]\n","tokenizer.decode(generated_ids)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":3521629,"sourceId":6146260,"sourceType":"datasetVersion"}],"dockerImageVersionId":30648,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":4}
