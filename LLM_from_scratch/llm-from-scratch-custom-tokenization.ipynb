{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7719458,"sourceType":"datasetVersion","datasetId":4508864}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This Kaggle Notebook serves as an enhanced version of its predecessor, incorporating a refined tokenization approach. Unlike the initial method, which simply divided words into their constituent characters, this notebook employs byte-pair encoding (BPE) for tokenization. By evaluating the frequency of each character and retaining the top 150 most common ones, the BPE process is then executed to expand the encoding, aiming for a targeted vocabulary size of 500. For a detailed understanding of the tokenizer's development and the mapping from text to tokens, refer to the dedicated Tokenizer Notebook. Further insights and foundational concepts can be explored through the resources listed below:\n\n- [From Zero to Hero in Natural Language Processing by Andrej Karpathy](https://karpathy.ai/zero-to-hero.html)\n- [A YouTube tutorial on the subject](https://www.youtube.com/watch?v=kCc8FmEb1nY)\n- [Previous Notebook iteration on Kaggle](https://www.kaggle.com/code/romainbdt/llm-from-scratch)\n- [Tokenizer Notebook on Kaggle](https://www.kaggle.com/code/romainbdt/tokenizer)","metadata":{}},{"cell_type":"markdown","source":"# Import libraries","metadata":{}},{"cell_type":"markdown","source":"In this section, we install necessary Python libraries, ensuring that our environment is prepared for the tasks ahead. The `datasets` library is updated to its latest version to take advantage of recent improvements and functionalities.\n","metadata":{}},{"cell_type":"code","source":"!pip install -U datasets","metadata":{"execution":{"iopub.status.busy":"2024-02-29T06:20:09.691582Z","iopub.execute_input":"2024-02-29T06:20:09.691878Z","iopub.status.idle":"2024-02-29T06:20:36.762145Z","shell.execute_reply.started":"2024-02-29T06:20:09.691852Z","shell.execute_reply":"2024-02-29T06:20:36.761062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Essential Python libraries for deep learning and data manipulation are imported here, including `torch` for model building and training, and `tqdm` for progress tracking during training.\n","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom tqdm import tqdm\nfrom datasets import load_dataset","metadata":{"execution":{"iopub.status.busy":"2024-02-29T06:20:36.764067Z","iopub.execute_input":"2024-02-29T06:20:36.764360Z","iopub.status.idle":"2024-02-29T06:20:39.834348Z","shell.execute_reply.started":"2024-02-29T06:20:36.764331Z","shell.execute_reply":"2024-02-29T06:20:39.833557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Parameters","metadata":{}},{"cell_type":"markdown","source":"This part of the notebook defines various parameters that will control the training process, dataset handling, and model architecture. These parameters include dataset splitting ratios, batch size, learning rate, and model-specific parameters like embedding size and dropout rate. Setting these parameters allows for fine-tuning the model's performance and training efficiency.\n","metadata":{}},{"cell_type":"code","source":"SEED = 15\n\n# Dataset parameters\ntrain_ratio = 0.99\nsample_size = 100_000\n\n# Training parameters\nbatch_size = 256*5 # how many independent sequences will we process in parallel?\nblock_size = 256 # what is the maximum context length for predictions?\neval_interval = 2000\nlearning_rate = 3e-4\neval_iters = 50\nn_embd = 128\nn_head = 4\nn_layer = 2\ndropout = 0.2\nnum_epochs = 1\n\n# Move the model to the appropriate device (GPU if available, else CPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T07:17:48.824903Z","iopub.execute_input":"2024-02-29T07:17:48.825662Z","iopub.status.idle":"2024-02-29T07:17:48.832467Z","shell.execute_reply.started":"2024-02-29T07:17:48.825634Z","shell.execute_reply":"2024-02-29T07:17:48.831469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.manual_seed(SEED)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T06:20:39.878805Z","iopub.execute_input":"2024-02-29T06:20:39.879148Z","iopub.status.idle":"2024-02-29T06:20:39.894854Z","shell.execute_reply.started":"2024-02-29T06:20:39.879122Z","shell.execute_reply":"2024-02-29T06:20:39.893739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.device_count()","metadata":{"execution":{"iopub.status.busy":"2024-02-29T06:20:39.896492Z","iopub.execute_input":"2024-02-29T06:20:39.896821Z","iopub.status.idle":"2024-02-29T06:20:39.922100Z","shell.execute_reply.started":"2024-02-29T06:20:39.896796Z","shell.execute_reply":"2024-02-29T06:20:39.921117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load files","metadata":{}},{"cell_type":"markdown","source":"The dataset is loaded using the `datasets` library. This example specifically loads a dataset from Wikimedia/Wikipedia, demonstrating how to work with real-world data sources.\n","metadata":{}},{"cell_type":"code","source":"%%time\n\n# Define the path to the dataset\ndataset_name = \"20231101.fr\"\n\n# Load the dataset\n# raw_dataset = load_dataset(\"wikimedia/wikipedia\", dataset_name, streaming=True)\nraw_dataset = load_dataset(\"wikimedia/wikipedia\", dataset_name)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T06:20:39.923290Z","iopub.execute_input":"2024-02-29T06:20:39.923565Z","iopub.status.idle":"2024-02-29T06:23:06.367938Z","shell.execute_reply.started":"2024-02-29T06:20:39.923542Z","shell.execute_reply":"2024-02-29T06:23:06.366964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_dataset['train'].num_rows","metadata":{"execution":{"iopub.status.busy":"2024-02-29T06:23:06.369381Z","iopub.execute_input":"2024-02-29T06:23:06.370234Z","iopub.status.idle":"2024-02-29T06:23:06.377078Z","shell.execute_reply.started":"2024-02-29T06:23:06.370199Z","shell.execute_reply":"2024-02-29T06:23:06.376161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data preparation","metadata":{}},{"cell_type":"markdown","source":"This section focuses on preparing the data for training. It includes steps such as dataset splitting into training and validation sets, tokenization, and conversion of text data into a format suitable for model training (e.g., converting text to integer sequences).\n","metadata":{}},{"cell_type":"code","source":"import json\nwith open(r'/kaggle/input/id-to-text-dictionnary-for-french-tokenization/MostCommonRegexTokenizer.txt', 'r') as f:\n    int_to_str = json.load(f)\n    \nint_to_str = {int(k):v for k,v in int_to_str.items()}\nvocab_size = len(int_to_str)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T06:23:06.378152Z","iopub.execute_input":"2024-02-29T06:23:06.379013Z","iopub.status.idle":"2024-02-29T06:23:16.332088Z","shell.execute_reply.started":"2024-02-29T06:23:06.378987Z","shell.execute_reply":"2024-02-29T06:23:16.331029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Create training and evaluation datasets\nif sample_size < 0:\n    train_sample = int(raw_dataset['train'].num_rows * train_ratio)\n    test_sample = int(raw_dataset['train'].num_rows * (1- train_ratio))\nelse:\n    train_sample = int(sample_size * train_ratio)\n    test_sample = int(sample_size * (1- train_ratio))\n    \nds_train_test = raw_dataset['train'].train_test_split(train_size=train_sample, test_size=test_sample, seed=SEED)\n\n# Convert Dataset into str using list comprehension and join method\ntrain_text = ''.join([item['text'] for item in ds_train_test['train']])\nval_text = ''.join([item['text'] for item in ds_train_test['test']])\n\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(train_text + val_text)))\nprint(\"number of unique chars:\", len(chars))\n\n# create a mapping from characters to integers\nstr_to_int = { ch:i for i,ch in int_to_str.items() }\n\nencode = lambda s: [str_to_int.get(c, 0) for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([int_to_str[i] for i in l]) # decoder: take a list of integers, output a string\n\n# Store datasets into torch tensors\ntrain_data = torch.tensor(encode(train_text), dtype=torch.long)\nval_data = torch.tensor(encode(val_text), dtype=torch.long)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T06:23:16.337204Z","iopub.execute_input":"2024-02-29T06:23:16.337663Z","iopub.status.idle":"2024-02-29T06:24:47.751530Z","shell.execute_reply.started":"2024-02-29T06:23:16.337620Z","shell.execute_reply":"2024-02-29T06:24:47.750578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data loading\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y","metadata":{"execution":{"iopub.status.busy":"2024-02-29T07:19:35.365495Z","iopub.execute_input":"2024-02-29T07:19:35.366193Z","iopub.status.idle":"2024-02-29T07:19:35.372289Z","shell.execute_reply.started":"2024-02-29T07:19:35.366159Z","shell.execute_reply":"2024-02-29T07:19:35.371350Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define loss function","metadata":{}},{"cell_type":"markdown","source":"Here, we define a custom loss function to evaluate the model's performance. This function will be used during training to guide the optimization process.\n","metadata":{}},{"cell_type":"code","source":"@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out","metadata":{"execution":{"iopub.status.busy":"2024-02-29T07:19:35.373854Z","iopub.execute_input":"2024-02-29T07:19:35.374394Z","iopub.status.idle":"2024-02-29T07:19:35.386127Z","shell.execute_reply.started":"2024-02-29T07:19:35.374363Z","shell.execute_reply":"2024-02-29T07:19:35.385220Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# GPT","metadata":{}},{"cell_type":"markdown","source":"The model architecture is defined in this section, including the implementation of a GPT-like language model with specific components such as multi-head attention, feed-forward networks, and layer normalization. This showcases the complexity and capability of the model being trained.\n","metadata":{}},{"cell_type":"code","source":"class Head(nn.Module):\n    \"\"\" one head of self-attention \"\"\"\n\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        # input of size (batch, time-step, channels)\n        # output of size (batch, time-step, head size)\n        B,T,C = x.shape\n        k = self.key(x)   # (B,T,hs)\n        q = self.query(x) # (B,T,hs)\n        # compute attention scores (\"affinities\")\n        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n        wei = F.softmax(wei, dim=-1) # (B, T, T)\n        wei = self.dropout(wei)\n        # perform the weighted aggregation of the values\n        v = self.value(x) # (B,T,hs)\n        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n        return out\n    \n    \nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(head_size * num_heads, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n    \n    \nclass FeedFoward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n    \nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n\n    def __init__(self, n_embd, n_head):\n        # n_embd: embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedFoward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n\n    \nclass GPTLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n        # better init, not covered in the original GPT video, but important, will cover in followup video\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def forward(self, idx, targets=None):\n#         if not isinstance(idx, torch.Tensor):\n#             print(type(idx))\n#             print(idx)\n        B, T = idx.shape\n\n        # idx and targets are both (B,T) tensor of integers\n        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n        x = tok_emb + pos_emb # (B,T,C)\n        x = self.blocks(x) # (B,T,C)\n        x = self.ln_f(x) # (B,T,C)\n        logits = self.lm_head(x) # (B,T,vocab_size)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # crop idx to the last block_size tokens\n            idx_cond = idx[:, -block_size:]\n            # get the predictions\n            logits, loss = self(idx_cond)\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # Set the probability of the <UNK> token (index 0) to zero\n            logits[:, 0] = float('-inf')  # This makes its probability effectively zero after softmax\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n","metadata":{"execution":{"iopub.status.busy":"2024-02-29T07:19:35.388122Z","iopub.execute_input":"2024-02-29T07:19:35.388541Z","iopub.status.idle":"2024-02-29T07:19:35.416229Z","shell.execute_reply.started":"2024-02-29T07:19:35.388510Z","shell.execute_reply":"2024-02-29T07:19:35.415225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomTextDataset(Dataset):\n    def __init__(self, encoded_text, block_size):\n        \"\"\"\n        Args:\n            encoded_text (Tensor): Encoded text data (as a long tensor).\n            block_size (int): The size of each sequence block to generate.\n        \"\"\"\n        self.data = encoded_text\n        self.block_size = block_size\n\n    def __len__(self):\n        # Subtract block_size to prevent overflow (can't create a block beyond the data length)\n        return len(self.data) - self.block_size\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Generates a single data pair (x, y).\n        \"\"\"\n        # Ensure that the index is within the valid range\n        idx = min(idx, len(self.data) - self.block_size - 1)\n        x = self.data[idx:idx+self.block_size]\n        y = self.data[idx+1:idx+self.block_size+1]\n        return x, y\n    \ndef get_dataloaders(train_dataset, val_dataset, batch_size: int = 64):    \n    # Instantiate the DataLoaders\n    train_dataloader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        drop_last=True,\n    )\n    val_dataloader = DataLoader(\n        val_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        drop_last=True,\n    )\n    return train_dataloader, val_dataloader","metadata":{"execution":{"iopub.status.busy":"2024-02-29T07:19:35.417473Z","iopub.execute_input":"2024-02-29T07:19:35.418310Z","iopub.status.idle":"2024-02-29T07:19:35.431208Z","shell.execute_reply.started":"2024-02-29T07:19:35.418284Z","shell.execute_reply":"2024-02-29T07:19:35.430355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = CustomTextDataset(train_data, block_size)\nval_dataset = CustomTextDataset(val_data, block_size)\ntrain_dl, eval_dl = get_dataloaders(train_dataset, val_dataset, batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T07:19:35.433997Z","iopub.execute_input":"2024-02-29T07:19:35.434597Z","iopub.status.idle":"2024-02-29T07:19:35.445584Z","shell.execute_reply.started":"2024-02-29T07:19:35.434566Z","shell.execute_reply":"2024-02-29T07:19:35.444801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\n\ndef generate_text(model, device, max_new_tokens):\n#     context = torch.zeros((1, 1), dtype=torch.long, device=device)\n#     context = torch.tensor([[2]], dtype=torch.long, device=device)  # Starting with token 2 -> \" \"\n    random_token_id = random.randint(0, vocab_size-1)  # Generate a random starting token within the vocabulary size\n    context = torch.tensor([[random_token_id]], dtype=torch.long, device=device)\n    \n    # Generate text\n    print(decode(model.generate(context, max_new_tokens=max_new_tokens)[0].tolist()))\n\n    \ndef check_device(model, batch):\n    print(\"Model device:\", next(model.parameters()).device)\n    for i, item in enumerate(batch):\n        print(f\"Batch item {i} device:\", item.device)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T07:19:35.446824Z","iopub.execute_input":"2024-02-29T07:19:35.447305Z","iopub.status.idle":"2024-02-29T07:19:35.456764Z","shell.execute_reply.started":"2024-02-29T07:19:35.447273Z","shell.execute_reply":"2024-02-29T07:19:35.455824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nmodel = GPTLanguageModel()\nmodel = model.to(device)\n# print the number of parameters in the model\nprint(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\n# Build the DataLoaders\ntrain_dataset = CustomTextDataset(train_data, block_size)\nval_dataset = CustomTextDataset(val_data, block_size)\ntrain_dataloader, val_dataloader = get_dataloaders(train_dataset, val_dataset, batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T07:19:35.457887Z","iopub.execute_input":"2024-02-29T07:19:35.458391Z","iopub.status.idle":"2024-02-29T07:19:35.496962Z","shell.execute_reply.started":"2024-02-29T07:19:35.458367Z","shell.execute_reply":"2024-02-29T07:19:35.496015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The training loop is where the model is actually trained. It iterates over the dataset, computes the loss, and updates the model's weights. This section also includes periodic evaluation on the validation set to monitor the model's performance.\n","metadata":{}},{"cell_type":"code","source":"%%time\n\n# And now we can train the model\nfor epoch in range(num_epochs):\n    model.train()\n    for step, batch in tqdm(enumerate(train_dataloader)):\n        batch = [item.to(device) for item in batch]  # Manually move to device\n#         check_device(model, batch)\n        \n        # every once in a while evaluate the loss on train and val sets\n        if step % eval_interval == 0:\n            losses = estimate_loss()\n            print(f\"step {step}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n            generate_text(model, device, max_new_tokens=100)\n            print('-' * 50)\n        \n        # evaluate the loss\n        logits, loss = model(batch[0], batch[1])\n        optimizer.zero_grad(set_to_none=True)\n        loss.backward()\n        optimizer.step()\n\n# generate from the model\ngenerate_text(model, device, max_new_tokens=500)\n#open('more.txt', 'w').write(decode(model.generate(context, max_new_tokens=10000)[0].tolist()))","metadata":{"execution":{"iopub.status.busy":"2024-02-29T07:19:35.498344Z","iopub.execute_input":"2024-02-29T07:19:35.498719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, this section provides optional steps for cleaning up GPU memory resources and summarizing the training process's outcomes. It's a good practice to release resources and provide a summary of the learned model's capabilities.","metadata":{}},{"cell_type":"code","source":"# torch.cuda.empty_cache()\n\n# import gc\n# del model # Replace 'variables' with the actual variables to delete\n# gc.collect()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}